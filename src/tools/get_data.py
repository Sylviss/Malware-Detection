import os, pickle, joblib
from torch import nn
from sklearn.preprocessing import StandardScaler,OneHotEncoder
import lightgbm as lgb
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression,LinearRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
import joblib as jl
from tqdm import tqdm
import numpy as np
from torch import nn
from skorch import NeuralNetClassifier
from sklearn.utils import resample
import torch
from tools.input import input_single_file_3
def get_model(model_str):
    match model_str:
        case 'Random Forest':
            return joblib.load("./models/RandomForestClassifier.joblib")
        case "LightGBM":
            return joblib.load("./models/LGBClassifier.joblib")
        case "Support Vector Machine":
            return joblib.load("./models/SVMClassifier.joblib")
        case "K-Nearest Neighbors":
            return joblib.load("./models/KNeighborsClassifier.joblib")
        case "Logistic Regression":
            return joblib.load("./models/LogisticRegression.joblib")
        case "Decision Tree":
            return joblib.load("./models/DecisionTreeClassifier.joblib")
        case "AdaBoost":
            return joblib.load("./models/AdaBoostClassifier.joblib")
        case "1 Layer Neural Network":
            from notebook.ann_1 import ann_1
            return ann_1()
        case "2 Layer Neural Network":
            from notebook.ann_2 import ann_2
            return ann_2()
        case "Convolutional Neural Network":
            from notebook.cnn import cnn
            return cnn()
        case "Ensemble Neural Network":
            from notebook.enn import enn
            return enn()
        
hehehehehehehehehehe = {0: "Benign", 1: "Malware"}        
        
def get_output(model,type,path):
    res = input_single_file_3(path)
    if res == {}:
        return "Error, file not found or not a PE file"
    for name in res:
        data = pd.DataFrame(data=res[name]["pe_header"],index = [0])
        match type:
            case 1:
                scaler = joblib.load("./tools/ScalerClassic.joblib")
                data_new = scaler.transform(data)
                prediction = model.predict(data_new)
                return hehehehehehehehehehe[prediction[0]]
            case 2:
                scaler = joblib.load("./tools/ScalerOhe.joblib")
                ohe = joblib.load("./tools/OheOhe.joblib")
                out = ["e_lfanew", "NumberOfSections", "BaseOfData", "SizeOfStackReserve", "SizeOfHeapReserve", "SizeOfHeapCommit", "E_text", "E_data", "filesize", "E_file"]
                categorical = data.drop(columns = out).to_numpy()
                non_categorical = data[out].to_numpy()
                trans_non_cate = scaler.transform(non_categorical)
                trans_cate = ohe.transform(categorical).toarray()
                data_new = np.concatenate((trans_cate, trans_non_cate), axis = 1)
                prediction = model.predict(data_new)
                return hehehehehehehehehehe[prediction[0]]
            case 3:
                scaler = joblib.load("./tools/ScalerOhe.joblib")
                ohe = joblib.load("./tools/OheOhe.joblib")
                out = ["e_lfanew", "NumberOfSections", "BaseOfData", "SizeOfStackReserve", "SizeOfHeapReserve", "SizeOfHeapCommit", "E_text", "E_data", "filesize", "E_file"]
                categorical = data.drop(columns = out).to_numpy()
                non_categorical = data[out].to_numpy()
                trans_non_cate = scaler.transform(non_categorical)
                trans_cate = ohe.transform(categorical).toarray()
                data_new = torch.tensor(np.concatenate((trans_cate, trans_non_cate), axis = 1),dtype=torch.float32)
                prediction = model.predict(data_new)
                return hehehehehehehehehehe[prediction[0]]
            case 4:
                scaler = joblib.load("./tools/ScalerOhe.joblib")
                ohe = joblib.load("./tools/OheOhe.joblib")
                ohc = OneHotEncoder()
                ohc.fit(np.array([0,1]).reshape(-1,1))
                out = ["e_lfanew", "NumberOfSections", "BaseOfData", "SizeOfStackReserve", "SizeOfHeapReserve", "SizeOfHeapCommit", "E_text", "E_data", "filesize", "E_file"]
                categorical = data.drop(columns = out).to_numpy()
                non_categorical = data[out].to_numpy()
                trans_non_cate = scaler.transform(non_categorical)
                trans_cate = ohe.transform(categorical).toarray()
                data_new = torch.tensor(np.concatenate((trans_cate, trans_non_cate), axis = 1),dtype=torch.float32).unsqueeze(1)
                prediction = model.predict(data_new)
                return hehehehehehehehehehe[ohc.inverse_transform(prediction)[0][0]]
            case 5:
                scaler = joblib.load("./tools/ScalerOhe.joblib")
                ohe = joblib.load("./tools/OheOhe.joblib")
                ohc = OneHotEncoder()
                ohc.fit(np.array([0,1]).reshape(-1,1))
                out = ["e_lfanew", "NumberOfSections", "BaseOfData", "SizeOfStackReserve", "SizeOfHeapReserve", "SizeOfHeapCommit", "E_text", "E_data", "filesize", "E_file"]
                categorical = data.drop(columns = out).to_numpy()
                non_categorical = data[out].to_numpy()
                trans_non_cate = scaler.transform(non_categorical)
                trans_cate = ohe.transform(categorical).toarray()
                data_new = torch.tensor(np.concatenate((trans_cate, trans_non_cate), axis = 1),dtype=torch.float32).unsqueeze(1)
                prediction = model.predict(data_new)
                return hehehehehehehehehehe[prediction[0]]
            case _:
                raise Exception("Invalid type")
    
    
def get_datas(type,seed):
    datasets = r"../Datasets/ClaMP_In_Selected.csv"
    df = pd.read_csv(datasets)
    df2 = pd.read_csv(r"../Datasets/ClaMP_In_Selected_Test.csv")
    df2 = df2.sample(frac = 1)
    # Seperate the data
    X_hehe, y = df.drop(columns=["class"]), df["class"].to_numpy()
    X_hehe_2, y_true = df2.drop(columns=["class"]), df2["class"].to_numpy()
    for i,label in enumerate(y):
        if label!=0:
            y[i]=1
    for i,label in enumerate(y_true):
        if label!=0:
            y_true[i]=1
    match type:
        case 1:
            scaler = StandardScaler()
            scaler.fit(X_hehe)
            X = scaler.transform(X_hehe)
            X_true = scaler.transform(X_hehe_2)
            X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = seed, test_size = 0.25)
            X_true, X_valid, y_true, y_valid = train_test_split(X_true,y_true, random_state = seed, test_size = 0.25)
            return X_train, X_test, y_train, y_test, X_true, y_true
        case 2:
            out = ["e_lfanew", "NumberOfSections", "BaseOfData", "SizeOfStackReserve", "SizeOfHeapReserve", "SizeOfHeapCommit", "E_text", "E_data", "filesize", "E_file"]
            scaler = StandardScaler()
            categories = [sorted(list(set(X_hehe[col]).union(set(X_hehe_2[col])))) for col in X_hehe.columns if col not in out]
            ohe = OneHotEncoder(categories=categories)
            categorical_1 = X_hehe.drop(columns = out).to_numpy()
            categorical_2 = X_hehe_2.drop(columns = out).to_numpy()
            non_categorical_1 = X_hehe[out].to_numpy()
            non_categorical_2 = X_hehe_2[out].to_numpy()
            scaler.fit(np.concatenate((non_categorical_1, non_categorical_2), axis = 0))
            trans_non_cate1 = scaler.transform(non_categorical_1)
            trans_non_cate2 = scaler.transform(non_categorical_2)
            ohe.fit(categorical_1)
            trans_cate_1 = ohe.transform(categorical_1).toarray()
            ohe.fit(categorical_2)
            trans_cate_2 = ohe.transform(categorical_2).toarray()
            X = np.concatenate((trans_cate_1, trans_non_cate1), axis = 1)
            X_true = np.concatenate((trans_cate_2, trans_non_cate2), axis = 1)
            X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = seed, test_size = 0.25)
            X_true, X_valid, y_true, y_valid = train_test_split(X_true,y_true, random_state = seed, test_size = 0.25)
            return X_train, X_test, y_train, y_test, X_true, y_true
        case 3:
            datasets = r"../Datasets/ClaMP_In_Selected.csv"
            df = pd.read_csv(datasets)
            df2 = pd.read_csv(r"../Datasets/ClaMP_In_Selected_Test.csv")
            df2 = df2.sample(frac = 1)
            # Seperate the data
            X_hehe,y = df.drop(columns=["class"]), df["class"].to_numpy()
            X_hehe_2, y_true = df2.drop(columns=["class"]), df2["class"].to_numpy()
            out = ["e_lfanew", "NumberOfSections", "BaseOfData", "SizeOfStackReserve", "SizeOfHeapReserve", "SizeOfHeapCommit", "E_text", "E_data", "filesize", "E_file"]
            scaler = StandardScaler()
            categories = [sorted(list(set(X_hehe[col]).union(set(X_hehe_2[col])))) for col in X_hehe.columns if col not in out]
            ohe = OneHotEncoder(categories=categories)
            categorical_1 = X_hehe.drop(columns = out).to_numpy()
            categorical_2 = X_hehe_2.drop(columns = out).to_numpy()
            non_categorical_1 = X_hehe[out].to_numpy()
            non_categorical_2 = X_hehe_2[out].to_numpy()
            scaler.fit(np.concatenate((non_categorical_1, non_categorical_2), axis = 0))
            trans_non_cate1 = scaler.transform(non_categorical_1)
            trans_non_cate2 = scaler.transform(non_categorical_2)
            ohe.fit(categorical_1)
            trans_cate_1 = ohe.transform(categorical_1).toarray()
            ohe.fit(categorical_2)
            trans_cate_2 = ohe.transform(categorical_2).toarray()
            X = np.concatenate((trans_cate_1, trans_non_cate1), axis = 1)
            X_true = np.concatenate((trans_cate_2, trans_non_cate2), axis = 1)
            X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 16, test_size = 0.25)
            X_true, X_valid, y_true, y_valid = train_test_split(X_true,y_true, random_state = 16, test_size = 0.25)
            for y in [y_train, y_test, y_valid, y_true]:
                for i,label in enumerate(y):
                    if label!=0:
                        y[i]=1
            X_train = torch.tensor(X_train, dtype=torch.float32)
            X_test = torch.tensor(X_test, dtype=torch.float32)
            X_valid = torch.tensor(X_valid, dtype=torch.float32)
            X_true = torch.tensor(X_true, dtype=torch.float32)
            y_train = torch.tensor(y_train, dtype=torch.float32)
            y_test = torch.tensor(y_test, dtype=torch.float32)
            y_valid = torch.tensor(y_valid, dtype=torch.float32)
            y_true = torch.tensor(y_true, dtype=torch.float32)
            return X_train, X_test, y_train, y_test, X_true, y_true
        case 4:
            datasets = r"../Datasets/ClaMP_In_Selected.csv"
            df = pd.read_csv(datasets)
            df2 = pd.read_csv(r"../Datasets/ClaMP_In_Selected_Test.csv")
            df2 = df2.sample(frac = 1)
            # Seperate the data
            X_hehe,y = df.drop(columns=["class"]), df["class"].to_numpy(np.float32)
            X_hehe_2, y_true = df2.drop(columns=["class"]), df2["class"].to_numpy(np.float32)
            for i,label in enumerate(y):
                if label!=0:
                    y[i]=1
            for i,label in enumerate(y_true):
                if label!=0:
                    y_true[i]=1
            ohc = OneHotEncoder()
            ohc.fit(y.reshape(-1, 1))
            y_ohe = ohc.transform(y.reshape(-1, 1)).toarray()
            y_true_ohe = ohc.transform(y_true.reshape(-1, 1)).toarray()
            out = ["e_lfanew", "NumberOfSections", "BaseOfData", "SizeOfStackReserve", "SizeOfHeapReserve", "SizeOfHeapCommit", "E_text", "E_data", "filesize", "E_file"]
            categories = [sorted(list(set(X_hehe[col]).union(set(X_hehe_2[col])))) for col in X_hehe.columns if col not in out]
            ohe = OneHotEncoder(categories=categories,handle_unknown="ignore")
            scaler = StandardScaler()
            categorical_1 = X_hehe.drop(columns = out).to_numpy()
            categorical_2 = X_hehe_2.drop(columns = out).to_numpy()
            non_categorical_1 = X_hehe[out].to_numpy()
            non_categorical_2 = X_hehe_2[out].to_numpy()
            scaler.fit(non_categorical_1)
            trans_non_cate1 = scaler.transform(non_categorical_1)
            trans_non_cate2 = scaler.transform(non_categorical_2)
            ohe.fit(categorical_1)
            trans_cate_1 = ohe.transform(categorical_1).toarray()
            ohe.fit(categorical_2)
            trans_cate_2 = ohe.transform(categorical_2).toarray()
            X = np.concatenate((trans_cate_1, trans_non_cate1), axis = 1)
            X_true_hehe = np.concatenate((trans_cate_2, trans_non_cate2), axis = 1)
            X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 16, test_size = 0.25)
            X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X,y_ohe, random_state = 16, test_size = 0.25)
            X_true, X_valid, y_true, y_valid = train_test_split(X_true_hehe,y_true, random_state = 16, test_size = 0.25)
            X_true, X_valid, y_true_ohe, y_valid_ohe = train_test_split(X_true_hehe,y_true_ohe, random_state = 16, test_size = 0.25)
            X_train = torch.from_numpy(X_train).unsqueeze(1).to(torch.float32)
            X_test = torch.from_numpy(X_test).unsqueeze(1).to(torch.float32)
            X_true = torch.from_numpy(X_true).unsqueeze(1).to(torch.float32)
            X_valid = torch.from_numpy(X_valid).unsqueeze(1).to(torch.float32)
            y_train_ohe = torch.from_numpy(y_train_ohe).to(torch.float32)
            y_test_ohe = torch.from_numpy(y_test_ohe).to(torch.float32)
            y_true_ohe = torch.from_numpy(y_true_ohe).to(torch.float32)
            y_valid_ohe = torch.from_numpy(y_valid).to(torch.float32)
            return X_train, X_test, y_train_ohe, y_test_ohe, X_true, y_true_ohe
        case 5:
            datasets = r"../Datasets/ClaMP_In_Selected.csv"
            df = pd.read_csv(datasets)
            df2 = pd.read_csv(r"../Datasets/ClaMP_In_Selected_Test.csv")
            df2 = df2.sample(frac = 1)
            # Seperate the data
            X_hehe,y = df.drop(columns=["class"]), df["class"].to_numpy(np.float32)
            X_hehe_2, y_true = df2.drop(columns=["class"]), df2["class"].to_numpy(np.float32)
            for i,label in enumerate(y):
                if label!=0:
                    y[i]=1
            for i,label in enumerate(y_true):
                if label!=0:
                    y_true[i]=1
            ohc = OneHotEncoder()
            ohc.fit(y.reshape(-1, 1))
            y_ohe = ohc.transform(y.reshape(-1, 1)).toarray()
            y_true_ohe = ohc.transform(y_true.reshape(-1, 1)).toarray()
            out = ["e_lfanew", "NumberOfSections", "BaseOfData", "SizeOfStackReserve", "SizeOfHeapReserve", "SizeOfHeapCommit", "E_text", "E_data", "filesize", "E_file"]
            categories = [sorted(list(set(X_hehe[col]).union(set(X_hehe_2[col])))) for col in X_hehe.columns if col not in out]
            ohe = OneHotEncoder(categories=categories,handle_unknown="ignore")
            scaler = StandardScaler()
            categorical_1 = X_hehe.drop(columns = out).to_numpy()
            categorical_2 = X_hehe_2.drop(columns = out).to_numpy()
            non_categorical_1 = X_hehe[out].to_numpy()
            non_categorical_2 = X_hehe_2[out].to_numpy()
            scaler.fit(non_categorical_1)
            trans_non_cate1 = scaler.transform(non_categorical_1)
            trans_non_cate2 = scaler.transform(non_categorical_2)
            ohe.fit(categorical_1)
            trans_cate_1 = ohe.transform(categorical_1).toarray()
            ohe.fit(categorical_2)
            trans_cate_2 = ohe.transform(categorical_2).toarray()
            X = np.concatenate((trans_cate_1, trans_non_cate1), axis = 1)
            X_true = np.concatenate((trans_cate_2, trans_non_cate2), axis = 1)
            X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 16, test_size = 0.25)
            X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X,y_ohe, random_state = 16, test_size = 0.25)
            X_train = torch.from_numpy(X_train).unsqueeze(1).to(torch.float32)
            X_test = torch.from_numpy(X_test).unsqueeze(1).to(torch.float32)
            X_true = torch.from_numpy(X_true).unsqueeze(1).to(torch.float32)
            y_train_ohe = torch.from_numpy(y_train_ohe).to(torch.float32)
            y_test_ohe = torch.from_numpy(y_test_ohe).to(torch.float32)
            y_true_ohe = torch.from_numpy(y_true_ohe).to(torch.float32)
            return X_train, X_test, y_train, y_test, X_true, y_true
        case _:
            raise Exception("WHYYY")